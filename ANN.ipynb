{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbyJV60QjxwZT4C1ElPE2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedabdelwahed2002/Artificial-Neural-Networks/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BNdyJZhaeWT",
        "outputId": "9ddab686-3fc9-4e14-9899-c544eae506d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 6.3903\n",
            "Epoch 100, Loss: 0.3824\n",
            "Epoch 200, Loss: 0.1553\n",
            "Epoch 300, Loss: 0.0644\n",
            "Epoch 400, Loss: 0.0351\n",
            "Epoch 500, Loss: 0.0262\n",
            "Epoch 600, Loss: 0.0225\n",
            "Epoch 700, Loss: 0.0201\n",
            "Epoch 800, Loss: 0.0182\n",
            "Epoch 900, Loss: 0.0166\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 0.0154, MAE: 0.0983, R2: 0.9873, Accuracy: 82.30%\n",
            "##################################################################\n",
            "Exponential Prediction Accuracy: 58.75%\n",
            "Exponential MSE: 0.0019\n",
            "\n",
            "Specific Test Samples:\n",
            "Input: [1. 2. 3.] | Prediction: -15.7018302869 | Ground Truth: -36.0000000000\n",
            "Input: [-3. -1.  2.] | Prediction: -10.9289014567 | Ground Truth: -23.0000000000\n",
            "Input: [ 0.5  1.5 -2.5] | Prediction: -11.9651092954 | Ground Truth: -23.5000000000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Define the target function\n",
        "def target_function(x, y, z):\n",
        "    return -(x**2 + 2*y**2 + 3*z**2)\n",
        "     #return np.exp(-(x**2 + 2*y**2 + 3*z**2))\n",
        "\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_data(num_samples=100000):\n",
        "    \"\"\"\n",
        "    Generates random data with inputs x, y, z in the range [-1, 1] and computes target outputs.\n",
        "    \"\"\"\n",
        "    x = np.random.uniform(-1, 1, size=(num_samples, 1))\n",
        "    y = np.random.uniform(-1, 1, size=(num_samples, 1))\n",
        "    z = np.random.uniform(-1, 1, size=(num_samples, 1))\n",
        "    inputs = np.hstack((x, y, z)) #Combines the individual arrays x, y, and z into a single 2D array where each row is a data point consisting of [x, y, z]\n",
        "    # hstack hena -> Horizontally stacks arrays (columns are placed side-by-side).\n",
        "    outputs = target_function(x, y, z)\n",
        "    return inputs, outputs # hena kol el inouts rag3a ka matrix say -> [[0.1, -0.5, 0.7],[x, y, z]] w osadhom outputs [-0.89, -0.97,......]\n",
        "\n",
        "# Neural Network definition\n",
        "#Encapsulates all the methods and attributes related to a feedforward neural network.\n",
        "#Provides a clean, reusable structure for implementing and training neural networks.\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size=3, hidden_size=64, output_size=1, num_hidden_layers=2, learning_rate=0.01):#Purpose: Initializes a new instance of the NeuralNetwork class.\n",
        "        # initializer 3ady gedan\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        # el kam satr el foo2 kol el bn3mlo bnset el variables (parameters el class)\n",
        "\n",
        "        # Disclaimer -> ma3loomat ethra2ya\n",
        "        # Xavier initialization is a technique for initializing the weights of a neural network in such a way\n",
        "        # that the variance of the activations remains consistent across all layers of the network.\n",
        "        # It helps prevent exploding gradients and vanishing gradients, which are common issues in deep networks.\n",
        "\n",
        "        # Xavier Initialization for weights\n",
        "        # input layer\n",
        "        self.weights = [np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)]\n",
        "        self.biases = [np.zeros(hidden_size)] # Note here the biases are those of the first layer\n",
        "        # hidden layers\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            self.weights.append(np.random.randn(hidden_size, hidden_size) * np.sqrt(1 / hidden_size))\n",
        "            self.biases.append(np.zeros(hidden_size))\n",
        "        # Initializes the weight matrix for the output layer and appends it to the self.weights list.\n",
        "        self.weights.append(np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size))\n",
        "        self.biases.append(np.zeros(output_size))\n",
        "\n",
        "    # ReLU activation function __/\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x) # hanraga3 el maximum been x w 0 wad7a\n",
        "    #fawa2ed estekhdam el relu :\n",
        "    # ReLU introduces non-linearity in the neural network\n",
        "    # law nefteker f pattern recognition kan law el activation function linear\n",
        "    # fa keda ehna ma3amlnash ay haga it is equivalent to one stage\n",
        "    # Derivative of ReLU\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float) #return a bool 1 for x greater than 0 and 0 other wise\n",
        "    #The vanishing gradient problem occurs when the gradients\n",
        "    #of the loss function with respect to the network's weights\n",
        "    #become extremely small as they are propagated backward through the\n",
        "    #layers during training. This happens because the gradients diminish\n",
        "    #exponentially in deep networks, particularly when certain activation functions (like sigmoid or tanh) are used.\n",
        "    ########################### WHAT CAN RESULT IN A VANISHING GRADIENT ###########################\n",
        "    #Loss Function and Data with Small Values:\n",
        "    #If the ground truth values (y) are close to 0 (as in your function, where y = e^-(large value), the loss gradients can also be small because the difference between predictions and ground truth (e.g., MSE) might be small.\n",
        "    # Forward pass\n",
        "\n",
        "\n",
        "\n",
        "# Feedforward Neural Network Architecture with Multiple Hidden Layers\n",
        "# =====================================================================\n",
        "# Input Layer          Hidden Layers                        Output Layer\n",
        "# (3 neurons: x, y, z) (e.g., [5, 10, 5] neurons)           (1 neuron: f)\n",
        "#\n",
        "#    [x]                [H1]          [H6]                 [Output]\n",
        "#    [y] ---> W1 --->   [H2] ---> W2  [H7] ---> W3 --->    [f(x, y, z)]\n",
        "#    [z]                [H3]          [H8]\n",
        "#                       [H4]          [H9]\n",
        "#                       [H5]          [H10]\n",
        "#\n",
        "# Explanation of Variables:\n",
        "# -------------------------\n",
        "# - W1: Weight matrix connecting the input layer (3 neurons) to the first hidden layer (e.g., 5 neurons).\n",
        "#       Shape: (3, 5)\n",
        "#       Controls: The strength of connections from the input layer to the first hidden layer.\n",
        "#\n",
        "# - b1: Bias vector added to the first hidden layer (e.g., 5 neurons).\n",
        "#       Shape: (5,)\n",
        "#       Controls: The bias added to each neuron in the first hidden layer.\n",
        "#\n",
        "# - W2: Weight matrix connecting the first hidden layer to the second hidden layer (e.g., 5 -> 10 neurons).\n",
        "#       Shape: (5, 10)\n",
        "#       Controls: The strength of connections between hidden layers.\n",
        "#\n",
        "# - b2: Bias vector added to the second hidden layer (e.g., 10 neurons).\n",
        "#       Shape: (10,)\n",
        "#       Controls: The bias added to each neuron in the second hidden layer.\n",
        "#\n",
        "# - ...\n",
        "#       Repeat for additional hidden layers.\n",
        "#\n",
        "# - Wn: Weight matrix connecting the last hidden layer to the output layer.\n",
        "#       Shape: (number of neurons in the last hidden layer, 1)\n",
        "#       Controls: The strength of connections from the last hidden layer to the output layer.\n",
        "#\n",
        "# - bn: Bias scalar added to the output neuron.\n",
        "#       Shape: (1,)\n",
        "#       Controls: The bias added to the output neuron.\n",
        "#\n",
        "# =====================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [] #Initializes an empty list to store the activations for each layer during the forward pass\n",
        "        self.z_values = [] #Initializes an empty list to store the pre-activation values (z) for each layer during the forward pass\n",
        "        # z values are the raw outputs before applying the activation function (i.e., the result of X @ weights + biases).\n",
        "\n",
        "        # Kholaset el kholasa\n",
        "        # Input Layer                Hidden Layer                Output Layer\n",
        "        #   x                          z_values                      z\n",
        "        #   y         ─────────>    activation     ─────────>    activation\n",
        "        #   z                          z_values                      z\n",
        "\n",
        "        # Explanation:\n",
        "        # 1. Input Layer:\n",
        "        #    - The input features (x, y, z) are provided to the network.\n",
        "        #\n",
        "        # 2. Hidden Layer:\n",
        "        #    - z_values: Computed as the linear combination of inputs, weights, and biases.\n",
        "        #                (z = X @ weights + biases)\n",
        "        #    - activation: Result of applying the activation function (e.g., ReLU) to the z_values.\n",
        "        #\n",
        "        # 3. Output Layer:\n",
        "        #    - z: The pre-activation value at the output layer.\n",
        "        #    - activation: The final output of the network, often without an activation function in regression tasks.\n",
        "\n",
        "\n",
        "        # Input layer\n",
        "        z = np.dot(X, self.weights[0]) + self.biases[0]\n",
        "        self.z_values.append(z)#Stores the computed z values for the first hidden layer in the list self.z_values.\n",
        "        #Use: These values are needed later during backpropagation to compute gradients.\n",
        "        activation = self.relu(z) #These activations serve as inputs to the next layer during forward propagation\n",
        "        self.activations.append(activation) #These activations serve as inputs to the next layer during forward propagation\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, self.num_hidden_layers):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i] #Computes the pre-activation values for the neurons in the current hidden layer.\n",
        "            self.z_values.append(z)\n",
        "            activation = self.relu(z)\n",
        "            self.activations.append(activation)\n",
        "\n",
        "        # Output layer\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1] #hena bn dot multiply activations fel weights\n",
        "        self.z_values.append(z)\n",
        "        self.activations.append(z)  # No activation for regression\n",
        "        return z\n",
        "\n",
        "        #-1 accesses the last element.\n",
        "        #-2 accesses the second-to-last element.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Backpropagation\n",
        "\n",
        "\n",
        "\n",
        "# Neural Network Architecture and Backpropagation Explanation\n",
        "# -----------------------------------------------------------\n",
        "# Output Layer               Hidden Layers                    Input Layer\n",
        "#       [f(x, y, z)]         [H6]         [H3]               [x]\n",
        "#       <-- W3 <---          [H7] <--- W2 [H4] <--- W1 <---  [y]\n",
        "#       [Loss Gradient]      [H8]         [H5]               [z]\n",
        "#       <-- b3 <---          [H9] <--- b2 [H2] <--- b1 <---\n",
        "\n",
        "# Explanation of Variables:\n",
        "# -------------------------\n",
        "# - Wn: Weight matrix for the nth layer:\n",
        "#   - Example: W3 connects the last hidden layer (e.g., [H6, H7, H8]) to the output layer ([f]).\n",
        "#   - Updates: Gradually adjusted by gradients computed during backpropagation to minimize loss.\n",
        "#   - Shape: (Number of neurons in previous layer, Number of neurons in current layer).\n",
        "\n",
        "# - bn: Bias vector for the nth layer:\n",
        "#   - Example: b2 is added to the second hidden layer's activations (e.g., [H6, H7, H8]).\n",
        "#   - Updates: Adjusted by summing the error contributions for all samples.\n",
        "\n",
        "# Backpropagation Steps:\n",
        "# ----------------------\n",
        "\n",
        "# 1. **Output Layer Gradients**:\n",
        "#    - Compute the gradient of the loss with respect to the output layer predictions ([f(x, y, z)]).\n",
        "#    - Propagate these gradients backward to the final hidden layer through W3 and b3.\n",
        "\n",
        "#    Formula:\n",
        "#    dW3 = (activations from last hidden layer)^T @ (loss gradients)\n",
        "#    db3 = sum of loss gradients\n",
        "\n",
        "# 2. **Hidden Layers Gradients**:\n",
        "#    - For each hidden layer (e.g., [H6, H7, H8]):\n",
        "#      - Multiply incoming gradients by the transpose of Wn (e.g., W3.T) to propagate errors.\n",
        "#      - Compute element-wise gradients using the derivative of the activation function (e.g., ReLU).\n",
        "#      - Accumulate gradients for Wn and bn for parameter updates.\n",
        "\n",
        "#    Formula:\n",
        "#    dWn = (activations from previous layer)^T @ (current loss gradients)\n",
        "#    dbn = sum of current loss gradients\n",
        "#    current loss gradients *= ReLU_derivative(z_n)\n",
        "\n",
        "# 3. **Input Layer Gradients**:\n",
        "#    - For the input layer ([x, y, z]):\n",
        "#      - Compute gradients for W1 and b1 to propagate errors backward.\n",
        "#      - Use the derivative of the activation function to modulate the gradient flow.\n",
        "\n",
        "#    Formula:\n",
        "#    dW1 = (input features X)^T @ (current loss gradients)\n",
        "#    db1 = sum of current loss gradients\n",
        "#    current loss gradients *= ReLU_derivative(z_1)\n",
        "\n",
        "# 4. **Update Parameters**:\n",
        "#    - Adjust weights (Wn) and biases (bn) using computed gradients and a specified learning rate:\n",
        "#      Wn = Wn - learning_rate * dWn\n",
        "#      bn = bn - learning_rate * dbn\n",
        "\n",
        "# Direction of Gradient Flow:\n",
        "# ---------------------------\n",
        "# [Output Layer] <- W3 <- [Last Hidden Layer] <- W2 <- [Second Hidden Layer] <- W1 <- [Input Layer]\n",
        "\n",
        "# The gradients flow backward through each layer starting from the output, adjusting parameters layer by layer.\n",
        "\n",
        "\n",
        "\n",
        "    def backward(self, X, y, predictions):\n",
        "        # Allocate space to store gradients for each weight and bias in the network.\n",
        "        # Step 1: Initialize storage for gradients\n",
        "        # ----------------------------------------\n",
        "        # Allocate space for gradients of the weights (d_weights) and biases (d_biases).\n",
        "        # - d_weights[i]: Gradient of the loss function with respect to weights connecting layer i to layer i+1.\n",
        "        # - d_biases[i]: Gradient of the loss function with respect to biases at layer i+1.\n",
        "        # Gradients have the same shape as the corresponding weights and biases.\n",
        "\n",
        "        d_weights = [np.zeros_like(w) for w in self.weights] #derivative of the loss with respect to the weights.\n",
        "        d_biases = [np.zeros_like(b) for b in self.biases] #derivative of the biases\n",
        "\n",
        "        # Step 2: Compute gradient for the output layer\n",
        "        # ---------------------------------------------\n",
        "        # - Calculate the gradient of the loss function with respect to the predictions (output of the forward pass).\n",
        "        # - For Mean Squared Error (MSE) loss: loss_gradient = 2 * (predictions - y) / y.size\n",
        "        # - This represents the direction and magnitude of the error for each sample in the output layer.\n",
        "\n",
        "        # Output layer gradient\n",
        "        loss_gradient = 2 * (predictions - y) / y.size\n",
        "\n",
        "        # Compute gradients for weights and biases connecting the last hidden layer to the output layer.\n",
        "        # - d_weights[-1]: Weight gradients for the final layer.\n",
        "        # - d_biases[-1]: Bias gradients for the final layer.\n",
        "        d_weights[-1] = np.dot(self.activations[-2].T, loss_gradient)\n",
        "        d_biases[-1] = np.sum(loss_gradient, axis=0)\n",
        "\n",
        "\n",
        "        # Backpropagate the loss gradient to the last hidden layer.\n",
        "        # - Use the transpose of the final layer's weight matrix to propagate errors backward.\n",
        "        # - The result represents the gradients of the loss with respect to the activations of the last hidden layer.\n",
        "        loss_gradient = np.dot(loss_gradient, self.weights[-1].T)\n",
        "        # Why use .T (transpose)? Because we’re dealing with matrix multiplication, and activations are row vectors.\n",
        "        # Hidden layers gradient\n",
        "        # Step 3: Compute gradients for hidden layers\n",
        "        # -------------------------------------------\n",
        "        # Iterate backward through the hidden layers, from the last to the first.\n",
        "        for i in range(self.num_hidden_layers - 1, 0, -1):\n",
        "            # Modulate loss_gradient using the derivative of the activation function (e.g., ReLU).\n",
        "            # - ReLU derivative: 1 for z > 0, 0 otherwise.\n",
        "            loss_gradient *= self.relu_derivative(self.z_values[i])\n",
        "            # Compute gradients for weights and biases connecting the previous hidden layer to the current hidden layer.\n",
        "            # - d_weights[i]: Weight gradients for the current hidden layer.\n",
        "            # - d_biases[i]: Bias gradients for the current hidden layer.\n",
        "            d_weights[i] = np.dot(self.activations[i - 1].T, loss_gradient)# Shape: (previous_layer_size, current_layer_size)\n",
        "            d_biases[i] = np.sum(loss_gradient, axis=0)# Shape: (current_layer_size,)\n",
        "            loss_gradient = np.dot(loss_gradient, self.weights[i].T)\n",
        "\n",
        "        # Input layer gradient\n",
        "        # Modulate loss_gradient using the derivative of the activation function (ReLU) applied to the first hidden layer.\n",
        "        loss_gradient *= self.relu_derivative(self.z_values[0])\n",
        "        # Compute gradients for weights and biases connecting the input layer to the first hidden layer.\n",
        "        # - d_weights[0]: Weight gradients for the first hidden layer.\n",
        "        # - d_biases[0]: Bias gradients for the first hidden layer.\n",
        "        d_weights[0] = np.dot(X.T, loss_gradient)\n",
        "        d_biases[0] = np.sum(loss_gradient, axis=0)\n",
        "\n",
        "        # Step 5: Update weights and biases\n",
        "        # ---------------------------------\n",
        "        # Update each weight matrix and bias vector using gradient descent:\n",
        "        # - New weight = Old weight - (learning rate * gradient of weight).\n",
        "        # - New bias = Old bias - (learning rate * gradient of bias).\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * d_weights[i]\n",
        "            self.biases[i] -= self.learning_rate * d_biases[i]\n",
        "\n",
        "    # Training\n",
        "    def train(self, X, y, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self.forward(X)\n",
        "            self.backward(X, y, predictions)\n",
        "            loss = mean_squared_error(y, predictions)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    # Predict\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)\n",
        "\n",
        "def calculate_exponential_accuracy(real_outputs, predicted_exponents):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predicted exponents compared to real exponential values.\n",
        "    \"\"\"\n",
        "    predicted_exponentials = np.exp(predicted_exponents)\n",
        "    real_exponentials = np.exp(real_outputs)\n",
        "    mse = mean_squared_error(real_exponentials, predicted_exponentials)\n",
        "    accuracy = np.mean(np.abs(predicted_exponentials - real_exponentials) / real_exponentials < 0.1) * 100\n",
        "    print(f\"Exponential Prediction Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Exponential MSE: {mse:.4f}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate 100,000 samples\n",
        "    inputs, outputs = generate_data(10000)\n",
        "\n",
        "    # Split data into 80% training and 20% testing\n",
        "    split_index = int(0.8 * len(inputs))\n",
        "    train_inputs, test_inputs = inputs[:split_index], inputs[split_index:]\n",
        "    train_outputs, test_outputs = outputs[:split_index], outputs[split_index:]\n",
        "\n",
        "    # Reshape outputs to match neural network expectations\n",
        "    train_outputs = train_outputs.reshape(-1, 1)\n",
        "    test_outputs = test_outputs.reshape(-1, 1)\n",
        "\n",
        "    # Initialize and train the network\n",
        "    model = NeuralNetwork(input_size=3, hidden_size=64, output_size=1, num_hidden_layers=3, learning_rate=0.01)\n",
        "    model.train(train_inputs, train_outputs, epochs=1000)\n",
        "\n",
        "    # Predict on the entire test set\n",
        "    test_predictions = model.predict(test_inputs)\n",
        "\n",
        "    # Calculate metrics on the 20% test data\n",
        "    mse = mean_squared_error(test_outputs, test_predictions)\n",
        "    mae = mean_absolute_error(test_outputs, test_predictions)\n",
        "    r2 = r2_score(test_outputs, test_predictions)\n",
        "    # Calculate accuracy based on percentage error\n",
        "    percentage_error = np.abs((test_predictions - test_outputs) / test_outputs) * 100  # Percentage error for each prediction\n",
        "    accuracy = np.mean(percentage_error < 10) * 100  # Percentage of predictions within 10% error\n",
        "\n",
        "\n",
        "    # Display metrics\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    print(f\"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Show predictions for specific test samples\n",
        "    specific_samples = np.array([[1.0, 2.0, 3.0], [-3.0, -1.0, 2.0], [0.5, 1.5, -2.5]])\n",
        "    specific_predictions = model.predict(specific_samples)\n",
        "    specific_outputs = target_function(specific_samples[:, 0], specific_samples[:, 1], specific_samples[:, 2]).reshape(-1, 1)\n",
        "\n",
        "    print(f\"##################################################################\")\n",
        "    # Calculate exponential accuracy\n",
        "    calculate_exponential_accuracy(test_outputs, test_predictions)\n",
        "\n",
        "    print(\"\\nSpecific Test Samples:\")\n",
        "    for i in range(len(specific_samples)):\n",
        "        print(f\"Input: {specific_samples[i]} | Prediction: {specific_predictions[i][0]:.10f} | Ground Truth: {specific_outputs[i][0]:.10f}\")\n"
      ]
    }
  ]
}